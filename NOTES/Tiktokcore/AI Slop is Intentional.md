---
title: "AI Slop is Intentional"
date: 2025-04-23
source: tiktok
tags:
  - children
  - capitalism
  - systems
  - gender
  - culture
  - tech
read_time: 5 min
---

# AI Slop is Intentional

What if AI Slop isn't a mistake or an unintended consequence or a side effect of this massive option of AI? What if it is actually the primary thing that we will use AI to do and it is the primary cultural object of this moment in history? When people talk about using AI to replace types of activity or labor that are about talking to other humans doing things foreign with other humans, what they're proposing is not increased efficiency. What they're proposing is increased legibility and the difference there is that when you start counting something, counting a certain output.

If you're paying someone by the hour versus by like the unit produced, what you start to assess by begins to shift the way that the system works. We saw this with standardized testing in American schools. If you're Gen Z or Gen Alpha, you experience that phenomenon of people teaching to the test, which literally changed, you know, the ways that you understood the world, the ways that people learned how to read. In your workplace, you probably experience this incentive structure too, where people are working to accomplish a certain number, whatever that number is attached to.

And there's some resemblance to the actual thing you're doing, but often that goal will not be precisely exactly connected to like every facet of your job. And the thing about AI, large language models, whatever it might be, is there gollums built out of data. So they are built essentially out of spreadsheets that people made in the past. They're built out of that bird's eye view perspective on whatever the job is.

So if you have an AI be a teacher, what that AI will excel at is doing everything exactly by the book, because the AI has nothing in it but the book. And so the reason why people empower people who write the books that, you know, people have to teach from or work from or kind of set the metrics that people have to meet. The reason they love AI isn't that it's better at doing those jobs. It's that it does them more legibly.

So A, the numbers look better and people think, oh, that must be better because they only think about the numbers and then B, they're more controllable. And the book I've been thinking about a lot is this one by James C Scott, seeing like a state, which is kind of a classic, but he has this one example where he's talking about these forests in Germany back in 1800s. And like the old school forests look like this, you know, it was like all types of plants, all types of trees, bushes, animals, but they're like, we're using the forest for lumber. So we want to have like the best possible trees to chop down and make wood from.

And they did it by legibility, right? So they wanted like the same age in certain plots so they could know how much wood they yield. They wanted things that would be easy to water and plant and trim. And so they made all these forests that looked like this.

They looked like these long hallways of trees, like all planet in a grid, all very easy to manage and but within a few decades, the whole forest and all the trees started to die and they weren't getting any wood anymore. And it was because they over optimized for that legible outcome of having all the trees planted in a row, of having a predictable harvest and yield. And they forgot to realize that like the way a forest really works is you need all those other plants to like move the nutrients through the soil. They built this system of legibility of assessing it, checking it, optimizing it.

And then that got its own inertia and its own momentum. And it prevented them from actually seeing what was happening in the forests on the ground. And if we build AI in such a way that we're turning culture or the world or the economy into a plant forest like that, it's not going to be sustainable. In many ways, it's already not.

Your platforms are full of slop. But what's most profitable to them and what guarantees their power most in the short term is to make that plant forest is to reduce the amount of diversity and value happening in online cultural spaces. And the best way to do that is through slop, which is hyperlegible, which is that perfectly planted row of trees in addition to whatever else it is. Seeing it that way isn't to say all AI is automatically bad or like you shouldn't be allowed to plant trees in a straight line if you want to, it's just to say that you can't like overcorrect in that direction.

Like there needs to be a balance. And one of the threats of this technology and the ways that slop is being used is to overcorrect and to make it so that there will be no balance so that we are building a system that in the future will just continue to produce these same outcomes of hyperlegibility until it becomes a language that only they can read and that we are left out of. It will be so legible to the people on top and the computers and the AI's that it will not at all resemble what you and I actually speak to one or another and that's when we get into you know a series of dangers, an unsustainable system, and all types of issues.

---
*Source: [TikTok](https://www.tiktokv.com/share/video/7495817999799176470/)*
